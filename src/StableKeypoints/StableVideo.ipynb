{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81088081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image as PILImage\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import abc\n",
    "from diffusers import StableVideoDiffusionPipeline, DDIMScheduler\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3fc39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionControl(abc.ABC):\n",
    "    def __init__(self):\n",
    "        self.cur_step = 0\n",
    "        self.num_att_layers = -1\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "\n",
    "    def between_steps(self):\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def num_uncond_att_layers(self):\n",
    "        return 0\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(self, attn_dict, is_cross: bool, place_in_unet: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, attn_dict, is_cross: bool, place_in_unet: str):\n",
    "        attn_dict = self.forward(attn_dict, is_cross, place_in_unet)\n",
    "        return attn_dict['attn']\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.cur_att_layer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c710870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionStore(AttentionControl):\n",
    "    @staticmethod\n",
    "    def get_empty_store():\n",
    "        return {\"attn\": []}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AttentionStore, self).__init__()\n",
    "        self.step_store = self.get_empty_store()\n",
    "\n",
    "    def forward(self, attn_dict, is_cross: bool, place_in_unet: str):\n",
    "        # Simply store the attention map\n",
    "        self.step_store[\"attn\"].append(attn_dict['attn'])\n",
    "        return attn_dict\n",
    "\n",
    "    def reset(self):\n",
    "        super(AttentionStore, self).reset()\n",
    "        self.step_store = self.get_empty_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ef899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_attention_control(model, controller: AttentionControl, feature_upsample_res=256):\n",
    "    def ca_forward(self, place_in_unet):\n",
    "        # Save reference to output projection\n",
    "        to_out = self.to_out\n",
    "        if isinstance(to_out, nn.ModuleList):\n",
    "            to_out = self.to_out[0]  # handle case where to_out is in a ModuleList\n",
    "\n",
    "        def forward(x, context=None, mask=None):\n",
    "            batch_size, sequence_length, dim = x.shape\n",
    "            h = self.heads\n",
    "            is_cross = context is not None\n",
    "            context = context if is_cross else x\n",
    "            # Standard QKV computation\n",
    "            q = self.to_q(x)\n",
    "            k = self.to_k(context)\n",
    "            v = self.to_v(context)\n",
    "            # Reshape to [batch*heads, ...] for attention computation\n",
    "            q = self.reshape_heads_to_batch_dim(q)\n",
    "            k = self.reshape_heads_to_batch_dim(k)\n",
    "            v = self.reshape_heads_to_batch_dim(v)\n",
    "            # Compute attention scores\n",
    "            sim = torch.einsum(\"b i d, b j d -> b i j\", q, k) * self.scale\n",
    "            if mask is not None:\n",
    "                # Apply mask if provided\n",
    "                mask = mask.reshape(batch_size, -1)\n",
    "                max_neg_value = -torch.finfo(sim.dtype).max\n",
    "                mask = mask[:, None, :].repeat(h, 1, 1)\n",
    "                sim = sim.masked_fill(~mask, max_neg_value)\n",
    "            # Softmax to get attention probabilities\n",
    "            attn = torch.softmax(sim, dim=-1)\n",
    "            attn = attn.clone()  # clone to avoid modifying original\n",
    "            # Compute attention output\n",
    "            out = torch.matmul(attn, v)\n",
    "            # If this is a cross-attention and spatial size is small (e.g., 32x32 or less), upsample for keypoint detection\n",
    "            if is_cross and sequence_length <= feature_upsample_res**2 and len(controller.step_store[\"attn\"]) < 4:\n",
    "                # Determine spatial dimensions (H, W) of attention map\n",
    "                spatial = int(sequence_length**0.5)\n",
    "                if spatial * spatial == sequence_length:\n",
    "                    H = W = spatial\n",
    "                else:\n",
    "                    # Use stored latent dimensions if available (handles non-square or multi-frame)\n",
    "                    H = getattr(controller, \"latent_h\", spatial)\n",
    "                    W = getattr(controller, \"latent_w\", spatial)\n",
    "                # Reshape and upsample the attention query `x`\n",
    "                x_reshaped = x.reshape(batch_size, H, W, dim).permute(0, 3, 1, 2)\n",
    "                x_reshaped = F.interpolate(x_reshaped, size=(feature_upsample_res, feature_upsample_res),\n",
    "                                           mode=\"bicubic\", align_corners=False)\n",
    "                x_reshaped = x_reshaped.permute(0, 2, 3, 1).reshape(batch_size, -1, dim)\n",
    "                # Recompute Q and attention with upsampled spatial resolution\n",
    "                q_up = self.to_q(x_reshaped)\n",
    "                q_up = self.reshape_heads_to_batch_dim(q_up)\n",
    "                sim_up = torch.einsum(\"b i d, b j d -> b i j\", q_up, k) * self.scale\n",
    "                attn_up = torch.softmax(sim_up, dim=-1)\n",
    "                attn_up = attn_up.clone()\n",
    "                # Store the upsampled attention map in the controller\n",
    "                controller({\"attn\": attn_up}, is_cross, place_in_unet)\n",
    "            else:\n",
    "                # If not capturing or not cross, just store the current attention map\n",
    "                controller({\"attn\": attn}, is_cross, place_in_unet)\n",
    "            # Reshape and project output\n",
    "            out = self.reshape_batch_dim_to_heads(out)\n",
    "            return to_out(out)\n",
    "\n",
    "        return forward\n",
    "\n",
    "    # If no custom controller provided, use a dummy that passes through\n",
    "    if controller is None:\n",
    "        controller = AttentionControl()\n",
    "\n",
    "    # Recursively register attention control on all CrossAttention modules in the model\n",
    "    def register_recurse(net, count, place_in_unet):\n",
    "        if net.__class__.__name__ == \"CrossAttention\":\n",
    "            net.forward = ca_forward(net, place_in_unet)\n",
    "            return count + 1\n",
    "        elif hasattr(net, \"children\"):\n",
    "            for child in net.children():\n",
    "                count = register_recurse(child, count, place_in_unet)\n",
    "        return count\n",
    "\n",
    "    cross_att_count = 0\n",
    "    for name, module in model.named_children():\n",
    "        if \"up\" in name:  # focus on upsampling blocks' cross-attention\n",
    "            cross_att_count += register_recurse(module, 0, \"up\")\n",
    "    controller.num_att_layers = cross_att_count\n",
    "    assert cross_att_count != 0, \"No cross-attention layers found. Please check model or diffusers version.\"\n",
    "    return controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d483997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook function to capture input shapes before each forward pass of the U-Net\n",
    "def _unet_pre_forward_hook(module, inputs, controllers_dict, current_device, feature_upsample_res):\n",
    "    # This hook runs at the start of each U-Net forward call\n",
    "    latent = inputs[0]  # the latent tensor input to the U-Net\n",
    "    controller = controllers_dict[current_device]\n",
    "    # Store frame count and latent spatial size for use in attention hook\n",
    "    if latent.dim() == 5:\n",
    "        # Input shape (batch, frames, channels, height, width)\n",
    "        controller.frames = latent.shape[1]\n",
    "        controller.latent_h = latent.shape[3]\n",
    "        controller.latent_w = latent.shape[4]\n",
    "    elif latent.dim() == 4:\n",
    "        # Input shape (batch, channels, height, width) â€“ treat as 1 frame\n",
    "        controller.frames = 1\n",
    "        controller.latent_h = latent.shape[2]\n",
    "        controller.latent_w = latent.shape[3]\n",
    "    # Register the attention control hooks on this module (unet)\n",
    "    register_attention_control(module, controller, feature_upsample_res=feature_upsample_res)\n",
    "    return  # no modification to inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8b1806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Stable Video Diffusion pipeline and attention controllers for each GPU\n",
    "def load_ldm(model_name=\"stabilityai/stable-video-diffusion-img2vid\", feature_upsample_res=256):\n",
    "    # Use DDIM scheduler for deterministic output\n",
    "    scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\",\n",
    "                              clip_sample=False, set_alpha_to_one=False)\n",
    "    scheduler.set_timesteps(50)  # default number of DDIM steps\n",
    "    # Load the Stable Video Diffusion pipeline\n",
    "    ldm = StableVideoDiffusionPipeline.from_pretrained(\n",
    "        model_name, torch_dtype=torch.float16, variant=\"fp16\", scheduler=scheduler\n",
    "    )\n",
    "    ldm = ldm.to(device)\n",
    "    # Enable multi-GPU support if available\n",
    "    if device != \"cpu\" and torch.cuda.device_count() > 1:\n",
    "        ldm.unet = nn.DataParallel(ldm.unet)\n",
    "        ldm.vae = nn.DataParallel(ldm.vae)\n",
    "    # Freeze model parameters (we will optimize only the query embeddings for keypoints)\n",
    "    for param in ldm.vae.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in ldm.unet.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in getattr(ldm, \"text_encoder\", []).parameters():\n",
    "        param.requires_grad = False  # stable-video-diffusion has no text encoder\n",
    "    # Set up attention controllers per device (for DataParallel splits)\n",
    "    controllers = {}\n",
    "    if device != \"cpu\" and torch.cuda.device_count() > 1:\n",
    "        for dev_id in ldm.unet.device_ids:\n",
    "            dev = torch.device(\"cuda\", dev_id)\n",
    "            controller = AttentionStore()\n",
    "            controllers[dev] = controller\n",
    "            # Hook attention on the unet module (DataParallel splits model per device)\n",
    "            ldm.unet.module.register_forward_pre_hook(\n",
    "                lambda module, inp, dev=dev, controller=controller: _unet_pre_forward_hook(module, inp, controllers, dev, feature_upsample_res)\n",
    "            )\n",
    "    else:\n",
    "        dev = torch.device(device)\n",
    "        controller = AttentionStore()\n",
    "        controllers[dev] = controller\n",
    "        ldm.unet.register_forward_pre_hook(\n",
    "            lambda module, inp: _unet_pre_forward_hook(module, inp, controllers, dev, feature_upsample_res)\n",
    "        )\n",
    "    return ldm, controllers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce6c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stable video diffusion model and prepare attention controllers\n",
    "ldm, controllers = load_ldm()\n",
    "print(\"Model loaded on device:\", device)\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of keypoints to find\n",
    "num_keypoints = 10  # you can adjust this for different scenarios\n",
    "num_optimization_steps = 500  # number of optimization iterations (increase for complex data)\n",
    "batch_size = 1  # images per batch (use >1 for larger datasets if GPU memory allows)\n",
    "augment_degrees = 30\n",
    "augment_scale = (0.9, 1.1)\n",
    "augment_translate = (0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e85bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare image dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_root, image_size=512):\n",
    "        self.data_root = data_root\n",
    "        self.image_paths = sorted([os.path.join(data_root, fname) for fname in os.listdir(data_root) \n",
    "                                   if fname.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = PILImage.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        # Resize to target size\n",
    "        img = img.resize((self.image_size, self.image_size))\n",
    "        img = np.array(img).astype(np.float32) / 255.0  # normalize to [0,1]\n",
    "        # Return image and original index (for potential tracking)\n",
    "        return {\"img\": torch.from_numpy(img).permute(2, 0, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a69b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize random context embedding (num_words tokens of dimension 1024)\n",
    "def init_random_noise(device, num_words=1000, dim=1024):\n",
    "    return torch.randn(1, num_words, dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c19ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random affine transform with inverse (for equivariance loss)\n",
    "class RandomAffineWithInverse:\n",
    "    def __init__(self, degrees=30, scale=(0.9, 1.1), translate=(0.1, 0.1)):\n",
    "        self.degrees = degrees\n",
    "        self.scale = scale\n",
    "        self.translate = translate\n",
    "\n",
    "    def __call__(self, img_tensor):\n",
    "        # img_tensor shape: [batch, 3, H, W]\n",
    "        # Apply random affine to a batch of images using torchvision or manual\n",
    "        # Here we assume batch_size=1 for simplicity\n",
    "        import torchvision.transforms.functional as TF\n",
    "        angle = np.random.uniform(-self.degrees, self.degrees)\n",
    "        scale_factor = np.random.uniform(self.scale[0], self.scale[1])\n",
    "        max_dx = self.translate[0] * img_tensor.shape[2]\n",
    "        max_dy = self.translate[1] * img_tensor.shape[3]\n",
    "        translations = (np.random.uniform(-max_dx, max_dx), np.random.uniform(-max_dy, max_dy))\n",
    "        # Apply affine\n",
    "        img = TF.affine(img_tensor, angle=angle, translate=translations, scale=scale_factor, shear=0)\n",
    "        # Store transform parameters for inverse if needed\n",
    "        self.last_params = (angle, translations, scale_factor)\n",
    "        return img\n",
    "\n",
    "    def inverse(self, img_tensor):\n",
    "        # Apply inverse of last used transform\n",
    "        import torchvision.transforms.functional as TF\n",
    "        angle, translations, scale_factor = self.last_params\n",
    "        inv_angle = -angle\n",
    "        inv_scale = 1.0 / scale_factor if scale_factor != 0 else 1.0\n",
    "        inv_translations = (-translations[0] * inv_scale, -translations[1] * inv_scale)\n",
    "        img = TF.affine(img_tensor, angle=inv_angle, translate=inv_translations, scale=inv_scale, shear=0)\n",
    "        return img_tensor if img is None else img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb2077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "def sharpening_loss(attn_map, device=\"cuda\", sigma=1.0):\n",
    "    # Encourage attention map values to be either 0 or 1 (sharpen peaks)\n",
    "    # We use mean squared error with a target peaked distribution (gaussian peak)\n",
    "    # Target is a gaussian peak centered at the attention location (already selected by indices)\n",
    "    # Here attn_map is a subset of attention values at candidate keypoint locations\n",
    "    target = torch.ones_like(attn_map)\n",
    "    return F.mse_loss(attn_map, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equivariance_loss(attn_map, attn_map_transformed, transform, index):\n",
    "    # Ensure attention map on original and transformed images (after inverse transform) are similar\n",
    "    attn_map_orig = attn_map\n",
    "    # Inverse-transform the transformed attention map back to original frame\n",
    "    attn_map_trans_inv = transform.inverse(attn_map_transformed)[index]\n",
    "    return F.mse_loss(attn_map_orig, attn_map_trans_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9dab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: find top k peak candidates in attention map by sampling points proportional to attn intensity\n",
    "def find_top_k_gaussian(attn_map, num_samples, sigma=16, num_subjects=1):\n",
    "    # attn_map shape: [H, W]\n",
    "    # Flatten and sample indices weighted by attention values\n",
    "    B, H, W = attn_map.shape[0], attn_map.shape[1], attn_map.shape[2]\n",
    "    flat_attn = attn_map.reshape(B, -1)\n",
    "    # Sample indices with probability proportional to attention\n",
    "    flat_attn = flat_attn + 1e-8\n",
    "    probs = (flat_attn / flat_attn.sum(dim=1, keepdim=True)).cpu().numpy()\n",
    "    indices = []\n",
    "    for b in range(B):\n",
    "        idx = np.random.choice(flat_attn.shape[1], size=num_samples, p=probs[b])\n",
    "        indices.append(torch.from_numpy(idx).long().to(attn_map.device))\n",
    "    return torch.stack(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: furthest point sampling to choose top_k distinct points from candidates\n",
    "def furthest_point_sampling(attn_map, top_k, candidate_indices):\n",
    "    # attn_map shape: [H, W]; candidate_indices: tensor of indices\n",
    "    # Here we just take the top_k highest values from candidate indices\n",
    "    # (In practice, one could implement FPS for diversity)\n",
    "    flat_attn = attn_map.reshape(-1)\n",
    "    vals = flat_attn[candidate_indices]\n",
    "    topk = torch.topk(vals, top_k)\n",
    "    return candidate_indices[topk.indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfbbe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and dataloader\n",
    "image_dir = \"./images\"  # directory containing input images/frames\n",
    "dataset = CustomDataset(data_root=image_dir, image_size=512)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "dataloader_iter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f9928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random context embedding with num_keypoints tokens\n",
    "context = init_random_noise(device, num_words=num_keypoints, dim=1024)\n",
    "context.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16902158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer for the embedding vectors\n",
    "optimizer = torch.optim.Adam([context], lr=5e-3)\n",
    "# Initialize random augmentation transform\n",
    "invertible_transform = RandomAffineWithInverse(degrees=augment_degrees, scale=augment_scale, translate=augment_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c837c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization loop\n",
    "for step in tqdm(range(num_optimization_steps)):\n",
    "    try:\n",
    "        batch = next(dataloader_iter)\n",
    "    except StopIteration:\n",
    "        dataloader_iter = iter(dataloader)\n",
    "        batch = next(dataloader_iter)\n",
    "    image = batch[\"img\"].to(device)  # shape [batch, 3, H, W], values in [0,1]\n",
    "    # Apply random augmentation\n",
    "    transformed_image = invertible_transform(image.clone())\n",
    "    # Run diffusion model for original and transformed images (one denoise step) to get attention maps\n",
    "    # We use our controllers to collect attention\n",
    "    attn_maps_list = []\n",
    "    attn_maps_trans_list = []\n",
    "    # Use the same noise level for both\n",
    "    noise_level = -1  # -1 will use the last scheduler timestep (i.e., final denoising step)\n",
    "    # Original image attention\n",
    "    with torch.no_grad():\n",
    "        # Encode image to latent\n",
    "        latent = None\n",
    "        if isinstance(ldm.vae, nn.DataParallel):\n",
    "            latent = ldm.vae.module.encode(image * 2 - 1)[\"latent_dist\"].mean.detach()\n",
    "        else:\n",
    "            latent = ldm.vae.encode(image * 2 - 1)[\"latent_dist\"].mean.detach()\n",
    "        latent = latent * (1/0.18215)  # scale factor used in stable diffusion models\n",
    "        noise = torch.randn_like(latent)\n",
    "        t = ldm.scheduler.timesteps[noise_level] if noise_level != -1 else ldm.scheduler.timesteps[-1]\n",
    "        noisy_latent = ldm.scheduler.add_noise(latent, noise, t)\n",
    "        # Forward through unet to predict noise (this triggers our hooks and fills controllers)\n",
    "        if isinstance(ldm.unet, nn.DataParallel):\n",
    "            pred = ldm.unet(noisy_latent, t.repeat(noisy_latent.shape[0]), encoder_hidden_states=context, added_time_ids=ldm._get_add_time_ids(7, 127, 0.02, latent.dtype, batch_size=noisy_latent.shape[0], num_videos_per_prompt=1, do_classifier_free_guidance=False))\n",
    "        else:\n",
    "            pred = ldm.unet(noisy_latent, t.repeat(noisy_latent.shape[0]), encoder_hidden_states=context, added_time_ids=ldm._get_add_time_ids(7, 127, 0.02, latent.dtype, batch_size=noisy_latent.shape[0], num_videos_per_prompt=1, do_classifier_free_guidance=False))\n",
    "    # Collect attention maps for this batch from all controllers (for multi-gpu, each controller holds part)\n",
    "    for ctrl in controllers.values():\n",
    "        # Collect and average attention maps from this controller\n",
    "        maps = ctrl.step_store[\"attn\"]\n",
    "        # Stack and average across layers and heads\n",
    "        if len(maps) > 0:\n",
    "            maps = torch.stack(maps, dim=0)  # shape [layers, b*h, seq, context]\n",
    "            attn_map = maps.mean(dim=(0,1))  # average over layers and heads\n",
    "            attn_map = attn_map[:, :1]  # focus on first context token if multiple (not used if we have multiple tokens directly)\n",
    "            attn_map = attn_map.reshape(1, int(attn_map.shape[0]**0.5), -1)\n",
    "            attn_maps_list.append(attn_map)\n",
    "        ctrl.reset()\n",
    "    # Transformed image attention\n",
    "    with torch.no_grad():\n",
    "        # Encode transformed image\n",
    "        latent_t = None\n",
    "        if isinstance(ldm.vae, nn.DataParallel):\n",
    "            latent_t = ldm.vae.module.encode(transformed_image * 2 - 1)[\"latent_dist\"].mean.detach()\n",
    "        else:\n",
    "            latent_t = ldm.vae.encode(transformed_image * 2 - 1)[\"latent_dist\"].mean.detach()\n",
    "        latent_t = latent_t * (1/0.18215)\n",
    "        noise_t = torch.randn_like(latent_t)\n",
    "        t = ldm.scheduler.timesteps[noise_level] if noise_level != -1 else ldm.scheduler.timesteps[-1]\n",
    "        noisy_latent_t = ldm.scheduler.add_noise(latent_t, noise_t, t)\n",
    "        # Forward unet\n",
    "        if isinstance(ldm.unet, nn.DataParallel):\n",
    "            pred_t = ldm.unet(noisy_latent_t, t.repeat(noisy_latent_t.shape[0]), encoder_hidden_states=context, added_time_ids=ldm._get_add_time_ids(7, 127, 0.02, latent_t.dtype, batch_size=noisy_latent_t.shape[0], num_videos_per_prompt=1, do_classifier_free_guidance=False))\n",
    "        else:\n",
    "            pred_t = ldm.unet(noisy_latent_t, t.repeat(noisy_latent_t.shape[0]), encoder_hidden_states=context, added_time_ids=ldm._get_add_time_ids(7, 127, 0.02, latent_t.dtype, batch_size=noisy_latent_t.shape[0], num_videos_per_prompt=1, do_classifier_free_guidance=False))\n",
    "    for ctrl in controllers.values():\n",
    "        if len(ctrl.step_store[\"attn\"]) > 0:\n",
    "            maps_t = torch.stack(ctrl.step_store[\"attn\"], dim=0)\n",
    "            attn_map_t = maps_t.mean(dim=(0,1))\n",
    "            attn_map_t = attn_map_t[:, :1]\n",
    "            attn_map_t = attn_map_t.reshape(1, int(attn_map_t.shape[0]**0.5), -1)\n",
    "            attn_maps_trans_list.append(attn_map_t)\n",
    "        ctrl.reset()\n",
    "    if not attn_maps_list or not attn_maps_trans_list:\n",
    "        continue  # skip if no attention maps collected (e.g., first few layers might not produce stored maps)\n",
    "    attn_maps = attn_maps_list[0]  # since batch_size=1 and one controller for simplicity\n",
    "    attn_maps_transformed = attn_maps_trans_list[0]\n",
    "    # Compute losses\n",
    "    _sharpening_loss = 0.0\n",
    "    _equiv_loss = 0.0\n",
    "    # Use the entire attention map to find candidate peaks\n",
    "    attn_map = attn_maps[0]  # shape [H, W]\n",
    "    attn_map_t = attn_maps_transformed[0]  # [H, W]\n",
    "    # Find candidate keypoint locations on original and transformed attention maps\n",
    "    candidates = find_top_k_gaussian(attn_map.unsqueeze(0), num_samples=50, sigma=16)\n",
    "    # Pick top K distinct points (furthest sampling or top values)\n",
    "    top_indices = furthest_point_sampling(attn_map, num_keypoints, candidates[0])\n",
    "    # Compute losses for these keypoint locations\n",
    "    _sharpening_loss = sharpening_loss(attn_map.view(-1)[top_indices])\n",
    "    _equiv_loss = equivariance_loss(attn_map.view(1, *attn_map.shape), attn_map_t.view(1, *attn_map_t.shape), invertible_transform, 0)\n",
    "    loss = _sharpening_loss + _equiv_loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61763396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After optimization, use the learned embeddings to get keypoint positions on each frame\n",
    "indices_by_frame = []\n",
    "image_paths = sorted([os.path.join(image_dir, fname) for fname in os.listdir(image_dir) \n",
    "                      if fname.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "image_w = image_h = 512  # we resized images to 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path in image_paths:\n",
    "    img = PILImage.open(img_path).convert(\"RGB\").resize((image_w, image_h))\n",
    "    img_tensor = torch.from_numpy(np.array(img).astype(np.float32) / 255.0).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "    frame_indices = []\n",
    "    # Compute attention map for each token\n",
    "    for token_idx in range(num_keypoints):\n",
    "        # Reset controllers\n",
    "        for ctrl in controllers.values():\n",
    "            ctrl.reset()\n",
    "        with torch.no_grad():\n",
    "            # Encode image to latent\n",
    "            latent = None\n",
    "            if isinstance(ldm.vae, nn.DataParallel):\n",
    "                latent = ldm.vae.module.encode(img_tensor * 2 - 1)[\"latent_dist\"].mean\n",
    "            else:\n",
    "                latent = ldm.vae.encode(img_tensor * 2 - 1)[\"latent_dist\"].mean\n",
    "            latent = latent * (1/0.18215)\n",
    "            noise = torch.randn_like(latent)\n",
    "            t = ldm.scheduler.timesteps[-1]  # final step\n",
    "            noisy_latent = ldm.scheduler.add_noise(latent, noise, t)\n",
    "            # Run unet for this token only (we provide only the token of interest via indices slicing)\n",
    "            # To isolate a single token, we create a context where that token is present and others maybe zeroed.\n",
    "            # Simplest: we pass the entire context, but collect only that token's attn in collect_maps by index.\n",
    "            if isinstance(ldm.unet, nn.DataParallel):\n",
    "                _ = ldm.unet(noisy_latent, t.repeat(noisy_latent.shape[0]), encoder_hidden_states=context,\n",
    "                             added_time_ids=ldm._get_add_time_ids(7, 127, 0.02, latent.dtype, batch_size=1, num_videos_per_prompt=1, do_classifier_free_guidance=False))\n",
    "            else:\n",
    "                _ = ldm.unet(noisy_latent, t.repeat(noisy_latent.shape[0]), encoder_hidden_states=context,\n",
    "                             added_time_ids=ldm._get_add_time_ids(7, 127, 0.02, latent.dtype, batch_size=1, num_videos_per_prompt=1, do_classifier_free_guidance=False))\n",
    "        # Gather attention maps from controller\n",
    "        attn_map_token = None\n",
    "        for ctrl in controllers.values():\n",
    "            if len(ctrl.step_store[\"attn\"]) > 0:\n",
    "                maps = torch.stack(ctrl.step_store[\"attn\"], dim=0)\n",
    "                attn = maps.mean(dim=(0,1))  # average over heads and layers\n",
    "                # Reshape to (H, W, context_length)\n",
    "                attn = attn.reshape(int(attn.shape[0]**0.5), -1, attn.shape[1])\n",
    "                # Take the map for the current token (token_idx)\n",
    "                if token_idx < attn.shape[2]:\n",
    "                    attn_map_token = attn[..., token_idx]\n",
    "                    break\n",
    "        if attn_map_token is None:\n",
    "            frame_indices.append(None)\n",
    "        else:\n",
    "            # Upsample attention map to image size\n",
    "            attn_map_token = attn_map_token.unsqueeze(0).unsqueeze(0)  # shape [1,1,H,W]\n",
    "            attn_map_token = F.interpolate(attn_map_token, size=(image_h, image_w), mode=\"bicubic\", align_corners=False)\n",
    "            attn_map_token = attn_map_token.squeeze()\n",
    "            # Find the peak coordinate\n",
    "            max_idx = torch.argmax(attn_map_token).item()\n",
    "            frame_indices.append(max_idx)\n",
    "    indices_by_frame.append(frame_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2779ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the resulting indices for each frame and keypoint\n",
    "print(\"Keypoint indices [frame][point]:\")\n",
    "for i, frame_indices in enumerate(indices_by_frame):\n",
    "    print(f\"Frame {i}: {frame_indices}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
