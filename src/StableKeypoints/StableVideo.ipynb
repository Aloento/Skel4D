{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "81088081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image as PILImage\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import abc\n",
    "from diffusers import StableVideoDiffusionPipeline, DDIMScheduler\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7d3fc39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionControl(abc.ABC):\n",
    "    def __init__(self):\n",
    "        self.cur_step = 0\n",
    "        self.num_att_layers = -1\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "\n",
    "    def between_steps(self):\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def num_uncond_att_layers(self):\n",
    "        return 0\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(self, attn_dict, is_cross: bool, place_in_unet: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, attn_dict, is_cross: bool, place_in_unet: str):\n",
    "        attn_dict = self.forward(attn_dict, is_cross, place_in_unet)\n",
    "        return attn_dict['attn']\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.cur_att_layer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c710870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionStore(AttentionControl):\n",
    "    @staticmethod\n",
    "    def get_empty_store():\n",
    "        return {\"attn\": []}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AttentionStore, self).__init__()\n",
    "        self.step_store = self.get_empty_store()\n",
    "\n",
    "    def forward(self, attn_dict, is_cross: bool, place_in_unet: str):\n",
    "        # Simply store the attention map\n",
    "        self.step_store[\"attn\"].append(attn_dict['attn'])\n",
    "        return attn_dict\n",
    "\n",
    "    def reset(self):\n",
    "        super(AttentionStore, self).reset()\n",
    "        self.step_store = self.get_empty_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a22ef899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_attention_control(model, controller: AttentionControl, feature_upsample_res=256):\n",
    "    def ca_forward(self, place_in_unet):\n",
    "        # Save reference to output projection\n",
    "        to_out = self.to_out\n",
    "        if isinstance(to_out, nn.ModuleList):\n",
    "            to_out = self.to_out[0]  # handle case where to_out is in a ModuleList\n",
    "\n",
    "        def forward(x, *args, **kwargs):\n",
    "            context = kwargs.get(\"encoder_hidden_states\", None)\n",
    "            mask = kwargs.get(\"mask\", None)\n",
    "            batch_size, sequence_length, dim = x.shape\n",
    "            h = self.heads\n",
    "            is_cross = context is not None\n",
    "            context = context if is_cross else x\n",
    "            # Standard QKV computation\n",
    "            q = self.to_q(x)\n",
    "            k = self.to_k(context)\n",
    "            v = self.to_v(context)\n",
    "\n",
    "            def reshape_heads_to_batch_dim(x):\n",
    "                # x shape: [batch, seq_len, d]\n",
    "                b, n, d = x.shape\n",
    "                head_dim = d // h\n",
    "                return x.view(b, n, h, head_dim).permute(0, 2, 1, 3).reshape(b * h, n, head_dim)\n",
    "\n",
    "            def reshape_batch_dim_to_heads(x):\n",
    "                # x shape: [batch*heads, seq_len, head_dim]\n",
    "                head_dim = x.shape[-1]\n",
    "                n = x.shape[1]\n",
    "                return x.view(batch_size, h, n, head_dim).permute(0, 2, 1, 3).reshape(batch_size, n, h * head_dim)\n",
    "\n",
    "            # Reshape to [batch*heads, ...] for attention computation\n",
    "            q = reshape_heads_to_batch_dim(q)\n",
    "            k = reshape_heads_to_batch_dim(k)\n",
    "            v = reshape_heads_to_batch_dim(v)\n",
    "            # Compute attention scores\n",
    "            sim = torch.einsum(\"b i d, b j d -> b i j\", q, k) * self.scale\n",
    "            if mask is not None:\n",
    "                # Apply mask if provided\n",
    "                mask = mask.reshape(batch_size, -1)\n",
    "                max_neg_value = -torch.finfo(sim.dtype).max\n",
    "                mask = mask[:, None, :].repeat(h, 1, 1)\n",
    "                sim = sim.masked_fill(~mask, max_neg_value)\n",
    "            # Softmax to get attention probabilities\n",
    "            attn = torch.softmax(sim, dim=-1)\n",
    "            attn = attn.clone()  # clone to avoid modifying original\n",
    "            # Compute attention output\n",
    "            out = torch.matmul(attn, v)\n",
    "            # If this is a cross-attention and spatial size is small (e.g., 32x32 or less), upsample for keypoint detection\n",
    "            if is_cross and sequence_length <= feature_upsample_res**2 and len(controller.step_store[\"attn\"]) < 4:\n",
    "                # Determine spatial dimensions (H, W) of attention map\n",
    "                spatial = int(sequence_length**0.5)\n",
    "                if spatial * spatial == sequence_length:\n",
    "                    H = W = spatial\n",
    "                else:\n",
    "                    # Use stored latent dimensions if available (handles non-square or multi-frame)\n",
    "                    H = getattr(controller, \"latent_h\", spatial)\n",
    "                    W = getattr(controller, \"latent_w\", spatial)\n",
    "                # Reshape and upsample the attention query `x`\n",
    "                x_reshaped = x.reshape(batch_size, H, W, dim).permute(0, 3, 1, 2)\n",
    "                x_reshaped = F.interpolate(x_reshaped, size=(feature_upsample_res, feature_upsample_res),\n",
    "                                           mode=\"bicubic\", align_corners=False)\n",
    "                x_reshaped = x_reshaped.permute(0, 2, 3, 1).reshape(batch_size, -1, dim)\n",
    "                # Recompute Q and attention with upsampled spatial resolution\n",
    "                q_up = self.to_q(x_reshaped)\n",
    "                q_up = reshape_heads_to_batch_dim(q_up)\n",
    "                sim_up = torch.einsum(\"b i d, b j d -> b i j\", q_up, k) * self.scale\n",
    "                attn_up = torch.softmax(sim_up, dim=-1)\n",
    "                attn_up = attn_up.clone()\n",
    "                # Store the upsampled attention map in the controller\n",
    "                controller({\"attn\": attn_up}, is_cross, place_in_unet)\n",
    "            else:\n",
    "                # If not capturing or not cross, just store the current attention map\n",
    "                controller({\"attn\": attn}, is_cross, place_in_unet)\n",
    "            # Reshape and project output\n",
    "            out = reshape_batch_dim_to_heads(out)\n",
    "            return to_out(out)\n",
    "\n",
    "        return forward\n",
    "\n",
    "    # If no custom controller provided, use a dummy that passes through\n",
    "    if controller is None:\n",
    "        controller = AttentionControl()\n",
    "\n",
    "    # Recursively register attention control on all Attention modules in the model\n",
    "    def register_recurse(net, count, place_in_unet):\n",
    "        if net.__class__.__name__ == \"Attention\":\n",
    "            net.forward = ca_forward(net, place_in_unet)\n",
    "            return count + 1\n",
    "        elif hasattr(net, \"children\"):\n",
    "            for child in net.children():\n",
    "                count = register_recurse(child, count, place_in_unet)\n",
    "        return count\n",
    "\n",
    "    cross_att_count = 0\n",
    "    for name, module in model.named_children():\n",
    "        if \"up\" in name:  # focus on upsampling blocks' cross-attention\n",
    "            cross_att_count += register_recurse(module, 0, \"up\")\n",
    "    controller.num_att_layers = cross_att_count\n",
    "    assert cross_att_count != 0, \"No attention layers found. Please check model or diffusers version.\"\n",
    "    print(f\"Registered {cross_att_count} attention layers for control.\")\n",
    "    return controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4d483997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook function to capture input shapes before each forward pass of the U-Net\n",
    "def _unet_pre_forward_hook(module, inputs, controllers_dict, current_device, feature_upsample_res):\n",
    "    # This hook runs at the start of each U-Net forward call\n",
    "    latent = inputs[0]  # the latent tensor input to the U-Net\n",
    "    controller = controllers_dict[current_device]\n",
    "    # Store frame count and latent spatial size for use in attention hook\n",
    "    if latent.dim() == 5:\n",
    "        # Input shape (batch, frames, channels, height, width)\n",
    "        controller.frames = latent.shape[1]\n",
    "        controller.latent_h = latent.shape[3]\n",
    "        controller.latent_w = latent.shape[4]\n",
    "    elif latent.dim() == 4:\n",
    "        # Input shape (batch, channels, height, width) â€“ treat as 1 frame\n",
    "        controller.frames = 1\n",
    "        controller.latent_h = latent.shape[2]\n",
    "        controller.latent_w = latent.shape[3]\n",
    "    # Register the attention control hooks on this module (unet)\n",
    "    register_attention_control(module, controller, feature_upsample_res=feature_upsample_res)\n",
    "    return  # no modification to inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4e8b1806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Stable Video Diffusion pipeline and attention controllers for each GPU\n",
    "def load_ldm(model_name=\"stabilityai/stable-video-diffusion-img2vid\", feature_upsample_res=256):\n",
    "    # Use DDIM scheduler for deterministic output\n",
    "    scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\",\n",
    "                              clip_sample=False, set_alpha_to_one=False)\n",
    "    scheduler.set_timesteps(50)  # default number of DDIM steps\n",
    "    # Load the Stable Video Diffusion pipeline\n",
    "    ldm = StableVideoDiffusionPipeline.from_pretrained(\n",
    "        model_name, torch_dtype=torch.float16, variant=\"fp16\", scheduler=scheduler\n",
    "    )\n",
    "    ldm = ldm.to(device)\n",
    "    # Enable multi-GPU support if available\n",
    "    if device != \"cpu\" and torch.cuda.device_count() > 1:\n",
    "        ldm.unet = nn.DataParallel(ldm.unet)\n",
    "        ldm.vae = nn.DataParallel(ldm.vae)\n",
    "    # Freeze model parameters (we will optimize only the query embeddings for keypoints)\n",
    "    for param in ldm.vae.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in ldm.unet.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Set up attention controllers per device (for DataParallel splits)\n",
    "    controllers = {}\n",
    "    if device != \"cpu\" and torch.cuda.device_count() > 1:\n",
    "        for dev_id in ldm.unet.device_ids:\n",
    "            dev = torch.device(\"cuda\", dev_id)\n",
    "            controller = AttentionStore()\n",
    "            controllers[dev] = controller\n",
    "            # Hook attention on the unet module (DataParallel splits model per device)\n",
    "            ldm.unet.module.register_forward_pre_hook(\n",
    "                lambda module, inp, dev=dev, controller=controller: _unet_pre_forward_hook(module, inp, controllers, dev, feature_upsample_res)\n",
    "            )\n",
    "    else:\n",
    "        dev = torch.device(device)\n",
    "        controller = AttentionStore()\n",
    "        controllers[dev] = controller\n",
    "        ldm.unet.register_forward_pre_hook(\n",
    "            lambda module, inp: _unet_pre_forward_hook(module, inp, controllers, dev, feature_upsample_res)\n",
    "        )\n",
    "    return ldm, controllers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5ce6c44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cuda\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# Load the stable video diffusion model and prepare attention controllers\n",
    "ldm, controllers = load_ldm()\n",
    "print(\"Model loaded on device:\", device)\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "26b36a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StableVideoDiffusionPipeline {\n",
      "  \"_class_name\": \"StableVideoDiffusionPipeline\",\n",
      "  \"_diffusers_version\": \"0.30.0.dev0\",\n",
      "  \"_name_or_path\": \"stabilityai/stable-video-diffusion-img2vid\",\n",
      "  \"feature_extractor\": [\n",
      "    \"transformers\",\n",
      "    \"CLIPImageProcessor\"\n",
      "  ],\n",
      "  \"image_encoder\": [\n",
      "    \"transformers\",\n",
      "    \"CLIPVisionModelWithProjection\"\n",
      "  ],\n",
      "  \"scheduler\": [\n",
      "    \"diffusers\",\n",
      "    \"DDIMScheduler\"\n",
      "  ],\n",
      "  \"unet\": [\n",
      "    \"diffusers\",\n",
      "    \"UNetSpatioTemporalConditionModel\"\n",
      "  ],\n",
      "  \"vae\": [\n",
      "    \"diffusers\",\n",
      "    \"AutoencoderKLTemporalDecoder\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "<__main__.AttentionStore object at 0x7f19381ec450>\n"
     ]
    }
   ],
   "source": [
    "print(ldm)\n",
    "print(controllers[torch.device(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3c80773f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNetSpatioTemporalConditionModel(\n",
      "  (conv_in): Conv2d(8, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (time_proj): Timesteps()\n",
      "  (time_embedding): TimestepEmbedding(\n",
      "    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "    (act): SiLU()\n",
      "    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (add_time_proj): Timesteps()\n",
      "  (add_embedding): TimestepEmbedding(\n",
      "    (linear_1): Linear(in_features=768, out_features=1280, bias=True)\n",
      "    (act): SiLU()\n",
      "    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (down_blocks): ModuleList(\n",
      "    (0): CrossAttnDownBlockSpatioTemporal(\n",
      "      (attentions): ModuleList(\n",
      "        (0-1): 2 x TransformerSpatioTemporalModel(\n",
      "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "          (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0): BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
      "                (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (temporal_transformer_blocks): ModuleList(\n",
      "            (0): TemporalBasicTransformerBlock(\n",
      "              (norm_in): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff_in): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
      "                (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (time_pos_embed): TimestepEmbedding(\n",
      "            (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "            (act): SiLU()\n",
      "            (linear_2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "          )\n",
      "          (time_proj): Timesteps()\n",
      "          (time_mixer): AlphaBlender()\n",
      "          (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x SpatioTemporalResBlock(\n",
      "          (spatial_res_block): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
      "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "          (temporal_res_block): TemporalResnetBlock(\n",
      "            (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (conv1): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
      "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "          (time_mixer): AlphaBlender()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): CrossAttnDownBlockSpatioTemporal(\n",
      "      (attentions): ModuleList(\n",
      "        (0-1): 2 x TransformerSpatioTemporalModel(\n",
      "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "          (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0): BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
      "                (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (temporal_transformer_blocks): ModuleList(\n",
      "            (0): TemporalBasicTransformerBlock(\n",
      "              (norm_in): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff_in): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
      "                (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (time_pos_embed): TimestepEmbedding(\n",
      "            (linear_1): Linear(in_features=640, out_features=2560, bias=True)\n",
      "            (act): SiLU()\n",
      "            (linear_2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "          )\n",
      "          (time_proj): Timesteps()\n",
      "          (time_mixer): AlphaBlender()\n",
      "          (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0): SpatioTemporalResBlock(\n",
      "          (spatial_res_block): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (temporal_res_block): TemporalResnetBlock(\n",
      "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (conv1): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "          (time_mixer): AlphaBlender()\n",
      "        )\n",
      "        (1): SpatioTemporalResBlock(\n",
      "          (spatial_res_block): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "          (temporal_res_block): TemporalResnetBlock(\n",
      "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (conv1): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "          (time_mixer): AlphaBlender()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): CrossAttnDownBlockSpatioTemporal(\n",
      "      (attentions): ModuleList(\n",
      "        (0-1): 2 x TransformerSpatioTemporalModel(\n",
      "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0): BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
      "                (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (temporal_transformer_blocks): ModuleList(\n",
      "            (0): TemporalBasicTransformerBlock(\n",
      "              (norm_in): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff_in): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
      "                (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (time_pos_embed): TimestepEmbedding(\n",
      "            (linear_1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (act): SiLU()\n",
      "            (linear_2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "          )\n",
      "          (time_proj): Timesteps()\n",
      "          (time_mixer): AlphaBlender()\n",
      "          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0): SpatioTemporalResBlock(\n",
      "          (spatial_res_block): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (temporal_res_block): TemporalResnetBlock(\n",
      "            (norm1): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "            (conv1): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "          (time_mixer): AlphaBlender()\n",
      "        )\n",
      "        (1): SpatioTemporalResBlock(\n",
      "          (spatial_res_block): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "          (temporal_res_block): TemporalResnetBlock(\n",
      "            (norm1): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "            (conv1): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "          (time_mixer): AlphaBlender()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): DownBlockSpatioTemporal(\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x SpatioTemporalResBlock(\n",
      "          (spatial_res_block): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "            (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "          (temporal_res_block): TemporalResnetBlock(\n",
      "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "            (conv1): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "          (time_mixer): AlphaBlender()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up_blocks): ModuleList(\n",
      "    (0): UpBlockSpatioTemporal(\n",
      "      (resnets): ModuleList(\n",
      "        (0-2): 3 x SpatioTemporalResBlock(\n",
      "          (spatial_res_block): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 2560, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (temporal_res_block): TemporalResnetBlock(\n",
      "            (norm1): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "            (conv1): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "          (time_mixer): AlphaBlender()\n",
      "        )\n",
      "      )\n",
      "      (upsamplers): ModuleList(\n",
      "        (0): Upsample2D(\n",
      "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): CrossAttnUpBlockSpatioTemporal(\n",
      "      (attentions): ModuleList(\n",
      "        (0-2): 3 x TransformerSpatioTemporalModel(\n",
      "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0): BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
      "                (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (temporal_transformer_blocks): ModuleList(\n",
      "            (0): TemporalBasicTransformerBlock(\n",
      "              (norm_in): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff_in): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
      "                (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (time_pos_embed): TimestepEmbedding(\n",
      "            (linear_1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (act): SiLU()\n",
      "            (linear_2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "          )\n",
      "          (time_proj): Timesteps()\n",
      "          (time_mixer): AlphaBlender()\n",
      "          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x SpatioTemporalResBlock(\n",
      "          (spatial_res_block): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 2560, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (temporal_res_block): TemporalResnetBlock(\n",
      "            (norm1): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "            (conv1): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "          (time_mixer): AlphaBlender()\n",
      "        )\n",
      "        (2): SpatioTemporalResBlock(\n",
      "          (spatial_res_block): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 1920, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (temporal_res_block): TemporalResnetBlock(\n",
      "            (norm1): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "            (conv1): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "            (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "          (time_mixer): AlphaBlender()\n",
      "        )\n",
      "      )\n",
      "      (upsamplers): ModuleList(\n",
      "        (0): Upsample2D(\n",
      "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): CrossAttnUpBlockSpatioTemporal(\n",
      "      (attentions): ModuleList(\n",
      "        (0-2): 3 x TransformerSpatioTemporalModel(\n",
      "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "          (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0): BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
      "                (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (temporal_transformer_blocks): ModuleList(\n",
      "            (0): TemporalBasicTransformerBlock(\n",
      "              (norm_in): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff_in): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
      "                (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
      "                (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (time_pos_embed): TimestepEmbedding(\n",
      "            (linear_1): Linear(in_features=640, out_features=2560, bias=True)\n",
      "            (act): SiLU()\n",
      "            (linear_2): Linear(in_features=2560, out_features=640, bias=True)\n",
      "          )\n",
      "          (time_proj): Timesteps()\n",
      "          (time_mixer): AlphaBlender()\n",
      "          (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0): SpatioTemporalResBlock(\n",
      "          (spatial_res_block): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 1920, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (temporal_res_block): TemporalResnetBlock(\n",
      "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (conv1): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "          (time_mixer): AlphaBlender()\n",
      "        )\n",
      "        (1): SpatioTemporalResBlock(\n",
      "          (spatial_res_block): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (temporal_res_block): TemporalResnetBlock(\n",
      "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (conv1): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "          (time_mixer): AlphaBlender()\n",
      "        )\n",
      "        (2): SpatioTemporalResBlock(\n",
      "          (spatial_res_block): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 960, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (temporal_res_block): TemporalResnetBlock(\n",
      "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (conv1): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
      "            (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv3d(640, 640, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "          (time_mixer): AlphaBlender()\n",
      "        )\n",
      "      )\n",
      "      (upsamplers): ModuleList(\n",
      "        (0): Upsample2D(\n",
      "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): CrossAttnUpBlockSpatioTemporal(\n",
      "      (attentions): ModuleList(\n",
      "        (0-2): 3 x TransformerSpatioTemporalModel(\n",
      "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "          (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0): BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
      "                (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (temporal_transformer_blocks): ModuleList(\n",
      "            (0): TemporalBasicTransformerBlock(\n",
      "              (norm_in): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff_in): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
      "                (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
      "                (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (time_pos_embed): TimestepEmbedding(\n",
      "            (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "            (act): SiLU()\n",
      "            (linear_2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "          )\n",
      "          (time_proj): Timesteps()\n",
      "          (time_mixer): AlphaBlender()\n",
      "          (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0): SpatioTemporalResBlock(\n",
      "          (spatial_res_block): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 960, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
      "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (temporal_res_block): TemporalResnetBlock(\n",
      "            (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (conv1): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
      "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "          (time_mixer): AlphaBlender()\n",
      "        )\n",
      "        (1-2): 2 x SpatioTemporalResBlock(\n",
      "          (spatial_res_block): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "            (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
      "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (temporal_res_block): TemporalResnetBlock(\n",
      "            (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (conv1): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
      "            (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): Conv3d(320, 320, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "          (time_mixer): AlphaBlender()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mid_block): UNetMidBlockSpatioTemporal(\n",
      "    (attentions): ModuleList(\n",
      "      (0): TransformerSpatioTemporalModel(\n",
      "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "        (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (transformer_blocks): ModuleList(\n",
      "          (0): BasicTransformerBlock(\n",
      "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn1): Attention(\n",
      "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_out): ModuleList(\n",
      "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn2): Attention(\n",
      "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
      "              (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
      "              (to_out): ModuleList(\n",
      "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (ff): FeedForward(\n",
      "              (net): ModuleList(\n",
      "                (0): GEGLU(\n",
      "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                )\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (temporal_transformer_blocks): ModuleList(\n",
      "          (0): TemporalBasicTransformerBlock(\n",
      "            (norm_in): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (ff_in): FeedForward(\n",
      "              (net): ModuleList(\n",
      "                (0): GEGLU(\n",
      "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                )\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn1): Attention(\n",
      "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_out): ModuleList(\n",
      "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn2): Attention(\n",
      "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
      "              (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
      "              (to_out): ModuleList(\n",
      "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (ff): FeedForward(\n",
      "              (net): ModuleList(\n",
      "                (0): GEGLU(\n",
      "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
      "                )\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (time_pos_embed): TimestepEmbedding(\n",
      "          (linear_1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "          (act): SiLU()\n",
      "          (linear_2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        )\n",
      "        (time_proj): Timesteps()\n",
      "        (time_mixer): AlphaBlender()\n",
      "        (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (resnets): ModuleList(\n",
      "      (0-1): 2 x SpatioTemporalResBlock(\n",
      "        (spatial_res_block): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "        (temporal_res_block): TemporalResnetBlock(\n",
      "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (conv1): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): Conv3d(1280, 1280, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "        (time_mixer): AlphaBlender()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "  (conv_act): SiLU()\n",
      "  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(ldm.unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7df5158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of keypoints to find\n",
    "num_keypoints = 10  # you can adjust this for different scenarios\n",
    "num_optimization_steps = 500  # number of optimization iterations (increase for complex data)\n",
    "batch_size = 1  # images per batch (use >1 for larger datasets if GPU memory allows)\n",
    "augment_degrees = 30\n",
    "augment_scale = (0.9, 1.1)\n",
    "augment_translate = (0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f6e85bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare image dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_root, image_size=512):\n",
    "        self.data_root = data_root\n",
    "        self.image_paths = sorted([os.path.join(data_root, fname) for fname in os.listdir(data_root) \n",
    "                                   if fname.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = PILImage.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        # Resize to target size\n",
    "        img = img.resize((self.image_size, self.image_size))\n",
    "        img = np.array(img).astype(np.float32) / 255.0  # normalize to [0,1]\n",
    "        # Return image and original index (for potential tracking)\n",
    "        return torch.from_numpy(img).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7a69b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize random context embedding (num_words tokens of dimension 1024)\n",
    "def init_random_noise(device, num_words=1000, dim=1024):\n",
    "    return torch.randn(1, num_words, dim, device=device, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e9c19ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random affine transform with inverse (for equivariance loss)\n",
    "class RandomAffineWithInverse:\n",
    "    def __init__(self, degrees=30, scale=(0.9, 1.1), translate=(0.1, 0.1)):\n",
    "        self.degrees = degrees\n",
    "        self.scale = scale\n",
    "        self.translate = translate\n",
    "\n",
    "    def __call__(self, img_tensor):\n",
    "        # img_tensor shape: [batch, 3, H, W]\n",
    "        # Apply random affine to a batch of images using torchvision or manual\n",
    "        # Here we assume batch_size=1 for simplicity\n",
    "        angle = np.random.uniform(-self.degrees, self.degrees)\n",
    "        scale_factor = np.random.uniform(self.scale[0], self.scale[1])\n",
    "        max_dx = self.translate[0] * img_tensor.shape[2]\n",
    "        max_dy = self.translate[1] * img_tensor.shape[3]\n",
    "        translations = (np.random.uniform(-max_dx, max_dx), np.random.uniform(-max_dy, max_dy))\n",
    "        # Apply affine\n",
    "        img = TF.affine(img_tensor, angle=angle, translate=translations, scale=scale_factor, shear=0)\n",
    "        # Store transform parameters for inverse if needed\n",
    "        self.last_params = (angle, translations, scale_factor)\n",
    "        return img\n",
    "\n",
    "    def inverse(self, img_tensor):\n",
    "        # Apply inverse of last used transform\n",
    "        angle, translations, scale_factor = self.last_params\n",
    "        inv_angle = -angle\n",
    "        inv_scale = 1.0 / scale_factor if scale_factor != 0 else 1.0\n",
    "        inv_translations = (-translations[0] * inv_scale, -translations[1] * inv_scale)\n",
    "        img = TF.affine(img_tensor, angle=inv_angle, translate=inv_translations, scale=inv_scale, shear=0)\n",
    "        return img_tensor if img is None else img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "beb2077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "def sharpening_loss(attn_map, device=\"cuda\", sigma=1.0):\n",
    "    # Encourage attention map values to be either 0 or 1 (sharpen peaks)\n",
    "    # We use mean squared error with a target peaked distribution (gaussian peak)\n",
    "    # Target is a gaussian peak centered at the attention location (already selected by indices)\n",
    "    # Here attn_map is a subset of attention values at candidate keypoint locations\n",
    "    target = torch.ones_like(attn_map)\n",
    "    return F.mse_loss(attn_map, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4392f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equivariance_loss(attn_map, attn_map_transformed, transform, index):\n",
    "    # Ensure attention map on original and transformed images (after inverse transform) are similar\n",
    "    attn_map_orig = attn_map\n",
    "    # Inverse-transform the transformed attention map back to original frame\n",
    "    attn_map_trans_inv = transform.inverse(attn_map_transformed)[index]\n",
    "    return F.mse_loss(attn_map_orig, attn_map_trans_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7b9dab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: find top k peak candidates in attention map by sampling points proportional to attn intensity\n",
    "def find_top_k_gaussian(attn_map, num_samples, sigma=16, num_subjects=1):\n",
    "    # attn_map shape: [H, W]\n",
    "    # Flatten and sample indices weighted by attention values\n",
    "    B, H, W = attn_map.shape[0], attn_map.shape[1], attn_map.shape[2]\n",
    "    flat_attn = attn_map.reshape(B, -1)\n",
    "    # Sample indices with probability proportional to attention\n",
    "    flat_attn = flat_attn + 1e-8\n",
    "    probs = (flat_attn / flat_attn.sum(dim=1, keepdim=True)).cpu().numpy()\n",
    "    indices = []\n",
    "    for b in range(B):\n",
    "        idx = np.random.choice(flat_attn.shape[1], size=num_samples, p=probs[b])\n",
    "        indices.append(torch.from_numpy(idx).long().to(attn_map.device))\n",
    "    return torch.stack(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0e1c63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: furthest point sampling to choose top_k distinct points from candidates\n",
    "def furthest_point_sampling(attn_map, top_k, candidate_indices):\n",
    "    # attn_map shape: [H, W]; candidate_indices: tensor of indices\n",
    "    # Here we just take the top_k highest values from candidate indices\n",
    "    # (In practice, one could implement FPS for diversity)\n",
    "    flat_attn = attn_map.reshape(-1)\n",
    "    vals = flat_attn[candidate_indices]\n",
    "    topk = torch.topk(vals, top_k)\n",
    "    return candidate_indices[topk.indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cbfbbe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and dataloader\n",
    "image_dir = \"/home/c_capzw/c_cape3d/data/rendered/objaverse/66a62fc9ab97415f85b6322c103f8e1e/Take001\"  # directory containing input images/frames\n",
    "dataset = CustomDataset(data_root=image_dir, image_size=512)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "dataloader_iter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e84f9928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random context embedding with num_keypoints tokens\n",
    "context = init_random_noise(device, num_words=num_keypoints, dim=1024)\n",
    "context.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "16902158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer for the embedding vectors\n",
    "optimizer = torch.optim.Adam([context], lr=5e-3)\n",
    "# Initialize random augmentation transform\n",
    "invertible_transform = RandomAffineWithInverse(degrees=augment_degrees, scale=augment_scale, translate=augment_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1c837c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered 36 attention layers for control.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 40.00 GiB. GPU 0 has a total capacity of 23.46 GiB of which 14.92 GiB is free. Including non-PyTorch memory, this process has 8.54 GiB memory in use. Of the allocated memory 7.20 GiB is allocated by PyTorch, and 1.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 41\u001b[0m\n\u001b[1;32m     36\u001b[0m         pred \u001b[38;5;241m=\u001b[39m ldm\u001b[38;5;241m.\u001b[39munet(noisy_latent,\n\u001b[1;32m     37\u001b[0m                         t\u001b[38;5;241m.\u001b[39mrepeat(noisy_latent\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m     38\u001b[0m                         encoder_hidden_states\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m     39\u001b[0m                         added_time_ids\u001b[38;5;241m=\u001b[39madded_time_ids)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m         pred \u001b[38;5;241m=\u001b[39m \u001b[43mldm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_latent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_latent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                        \u001b[49m\u001b[43madded_time_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madded_time_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Collect attention maps for this batch from all controllers (for multi-gpu, each controller holds part)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ctrl \u001b[38;5;129;01min\u001b[39;00m controllers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Collect and average attention maps from this controller\u001b[39;00m\n",
      "File \u001b[0;32m/project/c_cape3d/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/c_cape3d/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/project/c_cape3d/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1798\u001b[0m     ):\n\u001b[1;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/project/c_cape3d/.local/lib/python3.11/site-packages/diffusers/models/unets/unet_spatio_temporal_condition.py:464\u001b[0m, in \u001b[0;36mUNetSpatioTemporalConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, added_time_ids, return_dict)\u001b[0m\n\u001b[1;32m    461\u001b[0m down_block_res_samples \u001b[38;5;241m=\u001b[39m down_block_res_samples[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(upsample_block\u001b[38;5;241m.\u001b[39mresnets)]\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(upsample_block, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_cross_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m upsample_block\u001b[38;5;241m.\u001b[39mhas_cross_attention:\n\u001b[0;32m--> 464\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43mupsample_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mres_hidden_states_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_only_indicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_only_indicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    472\u001b[0m     sample \u001b[38;5;241m=\u001b[39m upsample_block(\n\u001b[1;32m    473\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39msample,\n\u001b[1;32m    474\u001b[0m         temb\u001b[38;5;241m=\u001b[39memb,\n\u001b[1;32m    475\u001b[0m         res_hidden_states_tuple\u001b[38;5;241m=\u001b[39mres_samples,\n\u001b[1;32m    476\u001b[0m         image_only_indicator\u001b[38;5;241m=\u001b[39mimage_only_indicator,\n\u001b[1;32m    477\u001b[0m     )\n",
      "File \u001b[0;32m/project/c_cape3d/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/c_cape3d/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/project/c_cape3d/.local/lib/python3.11/site-packages/diffusers/models/unets/unet_3d_blocks.py:2396\u001b[0m, in \u001b[0;36mCrossAttnUpBlockSpatioTemporal.forward\u001b[0;34m(self, hidden_states, res_hidden_states_tuple, temb, encoder_hidden_states, image_only_indicator)\u001b[0m\n\u001b[1;32m   2390\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2391\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m resnet(\n\u001b[1;32m   2392\u001b[0m             hidden_states,\n\u001b[1;32m   2393\u001b[0m             temb,\n\u001b[1;32m   2394\u001b[0m             image_only_indicator\u001b[38;5;241m=\u001b[39mimage_only_indicator,\n\u001b[1;32m   2395\u001b[0m         )\n\u001b[0;32m-> 2396\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimage_only_indicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_only_indicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2401\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2404\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m upsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers:\n",
      "File \u001b[0;32m/project/c_cape3d/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/c_cape3d/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/project/c_cape3d/.local/lib/python3.11/site-packages/diffusers/models/transformers/transformer_temporal.py:361\u001b[0m, in \u001b[0;36mTransformerSpatioTemporalModel.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, image_only_indicator, return_dict)\u001b[0m\n\u001b[1;32m    358\u001b[0m     hidden_states_mix \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    359\u001b[0m     hidden_states_mix \u001b[38;5;241m=\u001b[39m hidden_states_mix \u001b[38;5;241m+\u001b[39m emb\n\u001b[0;32m--> 361\u001b[0m     hidden_states_mix \u001b[38;5;241m=\u001b[39m \u001b[43mtemporal_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states_mix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_mixer(\n\u001b[1;32m    367\u001b[0m         x_spatial\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    368\u001b[0m         x_temporal\u001b[38;5;241m=\u001b[39mhidden_states_mix,\n\u001b[1;32m    369\u001b[0m         image_only_indicator\u001b[38;5;241m=\u001b[39mimage_only_indicator,\n\u001b[1;32m    370\u001b[0m     )\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# 3. Output\u001b[39;00m\n",
      "File \u001b[0;32m/project/c_cape3d/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/c_cape3d/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/project/c_cape3d/.local/lib/python3.11/site-packages/diffusers/models/attention.py:637\u001b[0m, in \u001b[0;36mTemporalBasicTransformerBlock.forward\u001b[0;34m(self, hidden_states, num_frames, encoder_hidden_states)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    636\u001b[0m     norm_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(hidden_states)\n\u001b[0;32m--> 637\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m attn_output \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    640\u001b[0m \u001b[38;5;66;03m# 4. Feed-forward\u001b[39;00m\n",
      "File \u001b[0;32m/project/c_cape3d/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/c_cape3d/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[135], line 61\u001b[0m, in \u001b[0;36mregister_attention_control.<locals>.ca_forward.<locals>.forward\u001b[0;34m(x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Reshape and upsample the attention query `x`\u001b[39;00m\n\u001b[1;32m     60\u001b[0m x_reshaped \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(batch_size, H, W, dim)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m x_reshaped \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeature_upsample_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_upsample_res\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbicubic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m x_reshaped \u001b[38;5;241m=\u001b[39m x_reshaped\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dim)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Recompute Q and attention with upsampled spatial resolution\u001b[39;00m\n",
      "File \u001b[0;32m/project/c_cape3d/.local/lib/python3.11/site-packages/torch/nn/functional.py:4707\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m antialias:\n\u001b[1;32m   4704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_upsample_bicubic2d_aa(\n\u001b[1;32m   4705\u001b[0m             \u001b[38;5;28minput\u001b[39m, output_size, align_corners, scale_factors\n\u001b[1;32m   4706\u001b[0m         )\n\u001b[0;32m-> 4707\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample_bicubic2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4708\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\n\u001b[1;32m   4709\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   4712\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot 3D input, but bilinear mode needs 4D input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 40.00 GiB. GPU 0 has a total capacity of 23.46 GiB of which 14.92 GiB is free. Including non-PyTorch memory, this process has 8.54 GiB memory in use. Of the allocated memory 7.20 GiB is allocated by PyTorch, and 1.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Optimization loop\n",
    "for step in tqdm(range(num_optimization_steps)):\n",
    "    try:\n",
    "        batch = next(dataloader_iter)\n",
    "    except StopIteration:\n",
    "        dataloader_iter = iter(dataloader)\n",
    "        batch = next(dataloader_iter)\n",
    "    image = batch.to(device)  # shape [batch, 3, H, W], values in [0,1]\n",
    "    # Apply random augmentation\n",
    "    transformed_images = invertible_transform(image.clone())\n",
    "    # Run diffusion model for original and transformed images (one denoise step) to get attention maps\n",
    "    # We use our controllers to collect attention\n",
    "    attn_maps_list = []\n",
    "    attn_maps_trans_list = []\n",
    "    # Use the same noise level for both\n",
    "    noise_level = -1  # -1 will use the last scheduler timestep (i.e., final denoising step)\n",
    "    # Original image attention\n",
    "    with torch.no_grad():\n",
    "        # Encode image to latent\n",
    "        latent = None\n",
    "        if isinstance(ldm.vae, nn.DataParallel):\n",
    "            latent = ldm.vae.module.encode((image * 2 - 1).to(torch.float16))[\"latent_dist\"].mean.detach()\n",
    "        else:\n",
    "            latent = ldm.vae.encode((image * 2 - 1).to(torch.float16))[\"latent_dist\"].mean.detach()\n",
    "        latent = latent * (1/0.18215)  # scale factor used in stable diffusion models\n",
    "        if latent.dim() == 4:\n",
    "            latent = latent.unsqueeze(1)  # [B, 1, 4, H, W]\n",
    "            latent = torch.cat([latent, latent], dim=2)  # [B, 1, 8, H, W]\n",
    "        noise = torch.randn_like(latent)\n",
    "        t = ldm.scheduler.timesteps[noise_level] if noise_level != -1 else ldm.scheduler.timesteps[-1]\n",
    "        t = t.to(device)\n",
    "        noisy_latent = ldm.scheduler.add_noise(latent, noise, t)\n",
    "        added_time_ids = ldm._get_add_time_ids(7, 127, 0.02, latent.dtype, batch_size=noisy_latent.shape[0], num_videos_per_prompt=1, do_classifier_free_guidance=False).to(torch.float16).to(device)\n",
    "        # Forward through unet to predict noise (this triggers our hooks and fills controllers)\n",
    "        if isinstance(ldm.unet, nn.DataParallel):\n",
    "            pred = ldm.unet(noisy_latent,\n",
    "                            t.repeat(noisy_latent.shape[0]),\n",
    "                            encoder_hidden_states=context,\n",
    "                            added_time_ids=added_time_ids)\n",
    "        else:\n",
    "            pred = ldm.unet(noisy_latent, \n",
    "                            t.repeat(noisy_latent.shape[0]), \n",
    "                            encoder_hidden_states=context, \n",
    "                            added_time_ids=added_time_ids)\n",
    "    # Collect attention maps for this batch from all controllers (for multi-gpu, each controller holds part)\n",
    "    for ctrl in controllers.values():\n",
    "        # Collect and average attention maps from this controller\n",
    "        maps = ctrl.step_store[\"attn\"]\n",
    "        # Stack and average across layers and heads\n",
    "        if len(maps) > 0:\n",
    "            maps = torch.stack(maps, dim=0)  # shape [layers, b*h, seq, context]\n",
    "            attn_map = maps.mean(dim=(0,1))  # average over layers and heads\n",
    "            attn_map = attn_map[:, :1]  # focus on first context token if multiple (not used if we have multiple tokens directly)\n",
    "            attn_map = attn_map.reshape(1, int(attn_map.shape[0]**0.5), -1)\n",
    "            attn_maps_list.append(attn_map)\n",
    "        ctrl.reset()\n",
    "    # Transformed image attention\n",
    "    with torch.no_grad():\n",
    "        # Encode transformed image\n",
    "        latent_t = None\n",
    "        if isinstance(ldm.vae, nn.DataParallel):\n",
    "            latent_t = ldm.vae.module.encode(transformed_images * 2 - 1)[\"latent_dist\"].mean.detach()\n",
    "        else:\n",
    "            latent_t = ldm.vae.encode(transformed_images * 2 - 1)[\"latent_dist\"].mean.detach()\n",
    "        latent_t = latent_t * (1/0.18215)\n",
    "        noise_t = torch.randn_like(latent_t)\n",
    "        t = ldm.scheduler.timesteps[noise_level] if noise_level != -1 else ldm.scheduler.timesteps[-1]\n",
    "        t = t.to(device)\n",
    "        noisy_latent_t = ldm.scheduler.add_noise(latent_t, noise_t, t)\n",
    "        added_time_ids = ldm._get_add_time_ids(7, 127, 0.02, latent_t.dtype, batch_size=noisy_latent_t.shape[0], num_videos_per_prompt=1, do_classifier_free_guidance=False).to(device)\n",
    "        # Forward unet\n",
    "        if isinstance(ldm.unet, nn.DataParallel):\n",
    "            pred_t = ldm.unet(noisy_latent_t,\n",
    "                              t.repeat(noisy_latent_t.shape[0]),\n",
    "                              encoder_hidden_states=context, \n",
    "                              added_time_ids=added_time_ids)\n",
    "        else:\n",
    "            pred_t = ldm.unet(noisy_latent_t, \n",
    "                              t.repeat(noisy_latent_t.shape[0]), \n",
    "                              encoder_hidden_states=context, \n",
    "                              added_time_ids=added_time_ids)\n",
    "    for ctrl in controllers.values():\n",
    "        if len(ctrl.step_store[\"attn\"]) > 0:\n",
    "            maps_t = torch.stack(ctrl.step_store[\"attn\"], dim=0)\n",
    "            attn_map_t = maps_t.mean(dim=(0,1))\n",
    "            attn_map_t = attn_map_t[:, :1]\n",
    "            attn_map_t = attn_map_t.reshape(1, int(attn_map_t.shape[0]**0.5), -1)\n",
    "            attn_maps_trans_list.append(attn_map_t)\n",
    "        ctrl.reset()\n",
    "    if not attn_maps_list or not attn_maps_trans_list:\n",
    "        continue  # skip if no attention maps collected (e.g., first few layers might not produce stored maps)\n",
    "    attn_maps = attn_maps_list[0]  # since batch_size=1 and one controller for simplicity\n",
    "    attn_maps_transformed = attn_maps_trans_list[0]\n",
    "    # Compute losses\n",
    "    _sharpening_loss = 0.0\n",
    "    _equiv_loss = 0.0\n",
    "    # Use the entire attention map to find candidate peaks\n",
    "    attn_map = attn_maps[0]  # shape [H, W]\n",
    "    attn_map_t = attn_maps_transformed[0]  # [H, W]\n",
    "    # Find candidate keypoint locations on original and transformed attention maps\n",
    "    candidates = find_top_k_gaussian(attn_map.unsqueeze(0), num_samples=50, sigma=16)\n",
    "    # Pick top K distinct points (furthest sampling or top values)\n",
    "    top_indices = furthest_point_sampling(attn_map, num_keypoints, candidates[0])\n",
    "    # Compute losses for these keypoint locations\n",
    "    _sharpening_loss = sharpening_loss(attn_map.view(-1)[top_indices])\n",
    "    _equiv_loss = equivariance_loss(attn_map.view(1, *attn_map.shape), attn_map_t.view(1, *attn_map_t.shape), invertible_transform, 0)\n",
    "    loss = _sharpening_loss + _equiv_loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61763396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After optimization, use the learned embeddings to get keypoint positions on each frame\n",
    "indices_by_frame = []\n",
    "image_paths = sorted([os.path.join(image_dir, fname) for fname in os.listdir(image_dir) \n",
    "                      if fname.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "image_w = image_h = 512  # we resized images to 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path in image_paths:\n",
    "    img = PILImage.open(img_path).convert(\"RGB\").resize((image_w, image_h))\n",
    "    img_tensor = torch.from_numpy(np.array(img).astype(np.float32) / 255.0).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "    frame_indices = []\n",
    "    # Compute attention map for each token\n",
    "    for token_idx in range(num_keypoints):\n",
    "        # Reset controllers\n",
    "        for ctrl in controllers.values():\n",
    "            ctrl.reset()\n",
    "        with torch.no_grad():\n",
    "            # Encode image to latent\n",
    "            latent = None\n",
    "            if isinstance(ldm.vae, nn.DataParallel):\n",
    "                latent = ldm.vae.module.encode(img_tensor * 2 - 1)[\"latent_dist\"].mean\n",
    "            else:\n",
    "                latent = ldm.vae.encode(img_tensor * 2 - 1)[\"latent_dist\"].mean\n",
    "            latent = latent * (1/0.18215)\n",
    "            noise = torch.randn_like(latent)\n",
    "            t = ldm.scheduler.timesteps[-1]  # final step\n",
    "            noisy_latent = ldm.scheduler.add_noise(latent, noise, t)\n",
    "            # Run unet for this token only (we provide only the token of interest via indices slicing)\n",
    "            # To isolate a single token, we create a context where that token is present and others maybe zeroed.\n",
    "            # Simplest: we pass the entire context, but collect only that token's attn in collect_maps by index.\n",
    "            if isinstance(ldm.unet, nn.DataParallel):\n",
    "                _ = ldm.unet(noisy_latent, t.repeat(noisy_latent.shape[0]), context=context,\n",
    "                             added_time_ids=ldm._get_add_time_ids(7, 127, 0.02, latent.dtype, batch_size=1, num_videos_per_prompt=1, do_classifier_free_guidance=False))\n",
    "            else:\n",
    "                _ = ldm.unet(noisy_latent, t.repeat(noisy_latent.shape[0]), context=context,\n",
    "                             added_time_ids=ldm._get_add_time_ids(7, 127, 0.02, latent.dtype, batch_size=1, num_videos_per_prompt=1, do_classifier_free_guidance=False))\n",
    "        # Gather attention maps from controller\n",
    "        attn_map_token = None\n",
    "        for ctrl in controllers.values():\n",
    "            if len(ctrl.step_store[\"attn\"]) > 0:\n",
    "                maps = torch.stack(ctrl.step_store[\"attn\"], dim=0)\n",
    "                attn = maps.mean(dim=(0,1))  # average over heads and layers\n",
    "                # Reshape to (H, W, context_length)\n",
    "                attn = attn.reshape(int(attn.shape[0]**0.5), -1, attn.shape[1])\n",
    "                # Take the map for the current token (token_idx)\n",
    "                if token_idx < attn.shape[2]:\n",
    "                    attn_map_token = attn[..., token_idx]\n",
    "                    break\n",
    "        if attn_map_token is None:\n",
    "            frame_indices.append(None)\n",
    "        else:\n",
    "            # Upsample attention map to image size\n",
    "            attn_map_token = attn_map_token.unsqueeze(0).unsqueeze(0)  # shape [1,1,H,W]\n",
    "            attn_map_token = F.interpolate(attn_map_token, size=(image_h, image_w), mode=\"bicubic\", align_corners=False)\n",
    "            attn_map_token = attn_map_token.squeeze()\n",
    "            # Find the peak coordinate\n",
    "            max_idx = torch.argmax(attn_map_token).item()\n",
    "            frame_indices.append(max_idx)\n",
    "    indices_by_frame.append(frame_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2779ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the resulting indices for each frame and keypoint\n",
    "print(\"Keypoint indices [frame][point]:\")\n",
    "for i, frame_indices in enumerate(indices_by_frame):\n",
    "    print(f\"Frame {i}: {frame_indices}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
