model:
  base_learning_rate: 1.0e-05
  target: zero123plus.model.MVDiffusion
  params:
    drop_cond_prob: 0.1

    # StableKeypoints configuration - ENABLED
    use_stable_keypoints: true
    num_learnable_tokens: 16
    sk_loss_weights:
      # Raw per-step losses observed: nvs≈0.0565, sharpening≈0.00176, equivariance≈2.4e-8
      # Target: bring weighted contributions roughly to nvs scale (~0.05–0.06)
      # weight ≈ 0.0565 / raw_loss. Sharpening weight ≈ 32. Equivariance would be ~2.3e6 (too large);
      # start with 1e5 and optionally ramp later if still negligible.
      sharpening: 30.0        # 0.00176 * 30 ≈ 0.053 (matched to nvs contribution)
      nvs: 1.0                # Keep baseline
      equivariance: 100000.0  # 2.4e-8 * 1e5 ≈ 2.4e-3 (small but no longer negligible); adjust after monitoring

    stable_diffusion_config:
      pretrained_model_name_or_path: sudo-ai/zero123plus-v1.2
      custom_pipeline: ./zero123plus
      # StableKeypoints parameters will be added automatically when use_stable_keypoints=true
      # use_learnable_embeddings: true (auto-added)
      # num_learnable_tokens: 16 (auto-added)

data:
  target: src.data.sk_objaverse.StableKeypointDataModuleFromConfig
  params:
    batch_size: 1  
    num_workers: 2
    train:
      target: src.data.sk_objaverse.StableKeypointObjaverseData
      params:
        root_dir: /objaverse/
        validation: false
    validation:
      target: src.data.sk_objaverse.StableKeypointObjaverseData
      params:
        root_dir: /objaverse/
        validation: true


lightning:
  modelcheckpoint:
    params:
      every_n_train_steps: 1000 #1000 #50
      save_top_k: -1
      save_last: true
  callbacks: {}

  trainer:
    benchmark: true
    max_epochs: -1
    gradient_clip_val: 1.0
    val_check_interval: 100000 #1000 #50
    num_sanity_val_steps: 0
    accumulate_grad_batches: 6  # Accumulate gradients due to smaller batch size
    check_val_every_n_epoch: null
