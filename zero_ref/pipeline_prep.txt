from typing import Any, Dict, Optional
from diffusers.models import AutoencoderKL, UNet2DConditionModel
from diffusers.schedulers import KarrasDiffusionSchedulers

import numpy
import torch
import torch.nn as nn
import transformers
from collections import OrderedDict
from PIL import Image
from torchvision import transforms
from transformers import CLIPImageProcessor, CLIPTextModel, CLIPTokenizer

import diffusers
from diffusers import (
    AutoencoderKL,
    DDPMScheduler,
    DiffusionPipeline,
    EulerAncestralDiscreteScheduler,
    UNet2DConditionModel,
    ImagePipelineOutput
)
from diffusers.image_processor import VaeImageProcessor
from diffusers.models.attention_processor import Attention, AttnProcessor, XFormersAttnProcessor, AttnProcessor2_0
from diffusers.utils.import_utils import is_xformers_available

def zero_module(module):
    """
    Zero out the parameters of a module and return it.
    """
    for p in module.parameters():
        p.detach().zero_()
    return module

class SkeletonConditioningNormalization(nn.Module):
    """
    Skeleton Conditioning Normalization (SCN) based on Skel3D paper.
    
    According to the Skel3D paper, SCN replaces Group Normalization:
    SCN(F_i, S) = GN(F_i) * (1 + γ) + β
    where γ and β are predicted from skeleton embeddings via zero-initialized convolution.
    """
    def __init__(self, in_channels, skeleton_latent_channels=4, eps=1e-5):
        super().__init__()
        self.in_channels = in_channels
        self.eps = eps
        
        # Group normalization (following the Skel3D practice)
        self.norm = nn.GroupNorm(32, in_channels, eps=eps, affine=False)
        
        # Zero-initialized convolution for skeleton modulation (following Skel3D)
        # This is key - zero initialization ensures stable training
        self.skeleton_layers = zero_module(
            nn.Conv2d(skeleton_latent_channels, 2 * in_channels, 1, padding=0)
        )
        
    def forward(self, x, skeleton_latents):
        """
        Apply Skel3D skeleton conditioning normalization.
        
        Args:
            x: Input features [B, C, H, W]
            skeleton_latents: Skeleton features [B, 4, H', W'] from VAE
        """
        # Apply group normalization first
        normalized = self.norm(x)  # GN(F_i) in Skel3D
        
        if skeleton_latents is None:
            return normalized
        
        # Interpolate skeleton latents to match feature resolution
        skel_resized = nn.functional.interpolate(
            skeleton_latents, 
            size=normalized.size()[-2:], 
            mode='bilinear', 
            align_corners=True
        )
        
        # Generate modulation parameters via zero-initialized convolution
        modulation = self.skeleton_layers(skel_resized)  # [B, 2*C, H, W]
        gamma, beta = modulation.chunk(2, dim=1)  # Each [B, C, H, W]
        
        # Apply skeleton conditioning: GN(F_i) * (1 + γ) + β
        return normalized * (1 + gamma) + beta


def to_rgb_image(maybe_rgba: Image.Image):
    if maybe_rgba.mode == 'RGB':
        return maybe_rgba
    elif maybe_rgba.mode == 'RGBA':
        rgba = maybe_rgba
        img = numpy.random.randint(255, 256, size=[rgba.size[1], rgba.size[0], 3], dtype=numpy.uint8)
        img = Image.fromarray(img, 'RGB')
        img.paste(rgba, mask=rgba.getchannel('A'))
        return img
    else:
        raise ValueError("Unsupported image type.", maybe_rgba.mode)


class ReferenceOnlyAttnProc(torch.nn.Module):
    def __init__(
        self,
        chained_proc,
        enabled=False,
        name=None,
        skel_enabled=False
    ) -> None:
        super().__init__()
        self.enabled = enabled
        self.chained_proc = chained_proc
        self.name = name
        self.skel_enabled = skel_enabled


    def __call__(
        self,
        attn: Attention,
        hidden_states,
        encoder_hidden_states=None,
        attention_mask=None,
        mode="w",
        ref_dict: dict = None,
        is_cfg_guidance = False
    ) -> Any:
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states

        # Note: Skeleton conditioning through SCN is now handled at ResNet block level
        # This attention processor focuses on reference-only attention
        
        # Handle CFG guidance
        if self.enabled and is_cfg_guidance:
            res0 = self.chained_proc(attn, hidden_states[:1], encoder_hidden_states[:1], attention_mask)
            hidden_states = hidden_states[1:]
            encoder_hidden_states = encoder_hidden_states[1:]
        if self.enabled:
            if mode == 'w':
                ref_dict[self.name] = encoder_hidden_states
            elif mode == 'r':
                encoder_hidden_states = torch.cat([encoder_hidden_states, ref_dict.pop(self.name)], dim=1)
            elif mode == 'm':
                encoder_hidden_states = torch.cat([encoder_hidden_states, ref_dict[self.name]], dim=1)
            else:
                assert False, mode
        res = self.chained_proc(attn, hidden_states, encoder_hidden_states, attention_mask)
        if self.enabled and is_cfg_guidance:
            res = torch.cat([res0, res])
        return res


class RefOnlyNoisedUNet(torch.nn.Module):
    def __init__(self, unet: UNet2DConditionModel,
                 train_sched: DDPMScheduler,
                 val_sched: EulerAncestralDiscreteScheduler,
                 skel_guidance=False) -> None:
        super().__init__()
        self.unet = unet
        self.train_sched = train_sched
        self.val_sched = val_sched

        unet_lora_attn_procs = dict()

        for name, _ in unet.attn_processors.items():
            if torch.__version__ >= '2.0':
                default_attn_proc = AttnProcessor2_0()
            elif is_xformers_available():
                default_attn_proc = XFormersAttnProcessor()
            else:
                default_attn_proc = AttnProcessor()

            # Determine if this layer should have conditioning
            enabled = name.endswith("attn1.processor")

            if enabled and skel_guidance:
                # Get hidden size from the attention layer for skeleton conditioning
                for module_name, module in unet.named_modules():
                    if name.replace('.processor', '') in module_name and hasattr(module, 'to_q'):
                        hidden_size = module.to_q.in_features
                        break
            
            unet_lora_attn_procs[name] = ReferenceOnlyAttnProc(
                default_attn_proc,
                enabled=name.endswith("attn1.processor"),
                name=name,
                skel_enabled=skel_guidance
            )
        
        unet.set_attn_processor(unet_lora_attn_procs)

        # Instead of a separate skeleton encoder, we'll encode skeleton through VAE
        # and apply skeleton conditioning normalization in the UNet blocks
        if skel_guidance:
            self._setup_skeleton_conditioning()

    def set_skeleton_latents(self, skeleton_latents):
        """Set skeleton latents for all ResNet blocks for SCN conditioning."""
        # Store skeleton latents in the pipeline instance instead of on individual blocks
        self._current_skeleton_latents = skeleton_latents
 
    def _setup_skeleton_conditioning(self):
        """
        Setup skeleton conditioning normalization layers in UNet blocks.
        Following Skel3D paper - SCN replaces specific normalization layers in ResNet blocks.
        """
        
        def add_scn_to_resnet_block(resnet_block):
            """Add SCN to a ResNet block, replacing norm2 as per Skel3D"""
            if not hasattr(resnet_block, 'norm1') or not hasattr(resnet_block, 'norm2'):
                return

            # Get the number of channels from norm2, not norm1!
            # SCN replaces norm2, so it must match norm2's expected input channels
            in_channels = resnet_block.norm2.num_channels
            
            # Capture original forward method for potential debugging
            original_forward = resnet_block.forward
            
            # Add SCN layer - this will replace norm2 in the ResNet block
            resnet_block.skeleton_norm = SkeletonConditioningNormalization(
                in_channels, skeleton_latent_channels=4
            )
            
            # Store original forward method for potential restoration
            resnet_block._original_forward = original_forward
            
            def forward_with_scn(hidden_states, temb=None, scale=1.0):
                """
                Modified ResNet forward pass with Skeleton Conditioning Normalization.
                Based on Skel3D paper Figure 2 and equation (2).
                """
                skeleton_latents = getattr(self, '_current_skeleton_latents', None)
                assert skeleton_latents is not None, "Skeleton latents must be set before forward pass"

                residual = hidden_states
                
                # First normalization and nonlinearity (UNCHANGED)
                hidden_states = resnet_block.norm1(hidden_states)
                hidden_states = resnet_block.nonlinearity(hidden_states)
                
                # Handle upsampling/downsampling (UNCHANGED)
                if resnet_block.upsample is not None:
                    if resnet_block.upsample.upsample_hidden_states:
                        hidden_states = resnet_block.upsample(hidden_states, scale=scale)
                        residual = resnet_block.upsample(residual, scale=scale)
                    else:
                        hidden_states = resnet_block.upsample(hidden_states, scale=scale)
                elif resnet_block.downsample is not None:
                    hidden_states = resnet_block.downsample(hidden_states, scale=scale)
                    residual = resnet_block.downsample(residual, scale=scale)
                
                # First convolution (UNCHANGED)
                hidden_states = resnet_block.conv1(hidden_states)
                
                # Add time embedding if present (UNCHANGED)
                if temb is not None:
                    if not resnet_block.skip_time_act:
                        temb = resnet_block.nonlinearity(temb)
                    temb = resnet_block.time_emb_proj(temb)[:, :, None, None]
                    hidden_states = hidden_states + temb
                
                # **KEY CHANGE** - Apply SCN instead of norm2
                # ORIGINAL: hidden_states = resnet_block.norm2(hidden_states)
                # MODIFIED: Apply skeleton conditioning normalization
                assert skeleton_latents is not None, "Skeleton latents must be provided for SCN"
                hidden_states = resnet_block.skeleton_norm(hidden_states, skeleton_latents)

                # Continue with standard ResNet (UNCHANGED)
                hidden_states = resnet_block.nonlinearity(hidden_states)
                hidden_states = resnet_block.dropout(hidden_states)
                hidden_states = resnet_block.conv2(hidden_states)
                
                # Shortcut connection (UNCHANGED)
                if resnet_block.conv_shortcut is not None:
                    residual = resnet_block.conv_shortcut(residual)
                
                # Final output with residual connection (UNCHANGED)
                output_tensor = (hidden_states + residual) / resnet_block.output_scale_factor
                
                return output_tensor
                
            # Replace the forward method
            resnet_block.forward = forward_with_scn
        
        # Apply SCN to all ResNet blocks in the UNet following Skel3D architecture
        # Down blocks
        for block in self.unet.down_blocks:
            if hasattr(block, 'resnets'):
                for resnet in block.resnets:
                    add_scn_to_resnet_block(resnet)
        
        # Middle block
        if hasattr(self.unet, 'mid_block') and hasattr(self.unet.mid_block, 'resnets'):
            for resnet in self.unet.mid_block.resnets:
                add_scn_to_resnet_block(resnet)
        
        # Up blocks
        for block in self.unet.up_blocks:
            if hasattr(block, 'resnets'):
                for resnet in block.resnets:
                    add_scn_to_resnet_block(resnet)

    def __getattr__(self, name: str):
        try:
            return super().__getattr__(name)
        except AttributeError:
            return getattr(self.unet, name)   
        
    def forward_cond(self, noisy_cond_lat, timestep, encoder_hidden_states, class_labels,
                     ref_dict, is_cfg_guidance, skel_latents=None, **kwargs):
        if is_cfg_guidance:
            encoder_hidden_states = encoder_hidden_states[1:]
            class_labels = class_labels[1:]
            #if skel_latents is not None:
            #    skel_latents = skel_latents[1:]

        # Set skeleton latents for SCN if available
        if skel_latents is not None:
            self.set_skeleton_latents(skel_latents)

        cross_attention_kwargs = dict(mode="w", ref_dict=ref_dict)
        if skel_latents is not None:
            cross_attention_kwargs['skel_latents'] = skel_latents
        else:
            assert False, "Skeleton latents must be provided for conditioning"

        self.unet(
            noisy_cond_lat, timestep,
            encoder_hidden_states=encoder_hidden_states,
            class_labels=class_labels,
            cross_attention_kwargs=cross_attention_kwargs,
            **kwargs
        )
        
        # Clear skeleton latents after forward pass
        if skel_latents is not None:
            self.set_skeleton_latents(None)

    def forward(
        self, sample, timestep, encoder_hidden_states, class_labels=None,
        *args, cross_attention_kwargs,
        down_block_res_samples=None, mid_block_res_sample=None,
        **kwargs
    ):
        cond_lat = cross_attention_kwargs['cond_lat']
        skel_latents = cross_attention_kwargs.get('skel_latents', None)
        is_cfg_guidance = cross_attention_kwargs.get('is_cfg_guidance', False)
        noise = torch.randn_like(cond_lat)
        if self.training:
            assert skel_latents is None, "Skeleton latents should not be provided during training"
            noisy_cond_lat = self.train_sched.add_noise(cond_lat, noise, timestep)
            noisy_cond_lat = self.train_sched.scale_model_input(noisy_cond_lat, timestep)
        else:
            assert skel_latents is not None, "Skeleton latents must be provided for validation"
            noisy_cond_lat = self.val_sched.add_noise(cond_lat, noise, timestep.reshape(-1))
            noisy_cond_lat = self.val_sched.scale_model_input(noisy_cond_lat, timestep.reshape(-1))
        ref_dict = {}
        self.forward_cond(
            noisy_cond_lat, timestep,
            encoder_hidden_states, class_labels,
            ref_dict, is_cfg_guidance, skel_latents, **kwargs
        )
        weight_dtype = self.unet.dtype
        cross_attention_kwargs_final = dict(
            mode="r", 
            ref_dict=ref_dict, 
            is_cfg_guidance=is_cfg_guidance
        )
        if skel_latents is not None:
            cross_attention_kwargs_final['skel_latents'] = skel_latents
            # Set skeleton latents for SCN
            self.set_skeleton_latents(skel_latents)

        result = self.unet(
            sample, timestep,
            encoder_hidden_states, *args,
            class_labels=class_labels,
            cross_attention_kwargs=cross_attention_kwargs_final,
            down_block_additional_residuals=[
                sample.to(dtype=weight_dtype) for sample in down_block_res_samples
            ] if down_block_res_samples is not None else None,
            mid_block_additional_residual=(
                mid_block_res_sample.to(dtype=weight_dtype)
                if mid_block_res_sample is not None else None
            ),
            **kwargs
        )
        
        # Clear skeleton latents after forward pass
        if skel_latents is not None:
            self.set_skeleton_latents(None)
            
        return result


def scale_latents(latents):
    latents = (latents - 0.22) * 0.75
    return latents


def unscale_latents(latents):
    latents = latents / 0.75 + 0.22
    return latents


def scale_image(image):
    image = image * 0.5 / 0.8
    return image


def unscale_image(image):
    image = image / 0.5 * 0.8
    return image

'''
class DepthControlUNet(torch.nn.Module):
    def __init__(self, unet: RefOnlyNoisedUNet, controlnet: Optional[diffusers.ControlNetModel] = None, conditioning_scale=1.0) -> None:
        super().__init__()
        self.unet = unet
        if controlnet is None:
            self.controlnet = diffusers.ControlNetModel.from_unet(unet.unet)
        else:
            self.controlnet = controlnet
        DefaultAttnProc = AttnProcessor2_0
        if is_xformers_available():
            DefaultAttnProc = XFormersAttnProcessor
        self.controlnet.set_attn_processor(DefaultAttnProc())
        self.conditioning_scale = conditioning_scale

    def __getattr__(self, name: str):
        try:
            return super().__getattr__(name)
        except AttributeError:
            return getattr(self.unet, name)

    def forward(self, sample, timestep, encoder_hidden_states, class_labels=None, *args, cross_attention_kwargs: dict, **kwargs):
        cross_attention_kwargs = dict(cross_attention_kwargs)
        control_depth = cross_attention_kwargs.pop('control_depth')
        down_block_res_samples, mid_block_res_sample = self.controlnet(
            sample,
            timestep,
            encoder_hidden_states=encoder_hidden_states,
            controlnet_cond=control_depth,
            conditioning_scale=self.conditioning_scale,
            return_dict=False,
        )
        return self.unet(
            sample,
            timestep,
            encoder_hidden_states=encoder_hidden_states,
            down_block_res_samples=down_block_res_samples,
            mid_block_res_sample=mid_block_res_sample,
            cross_attention_kwargs=cross_attention_kwargs
        )
'''

class ModuleListDict(torch.nn.Module):
    def __init__(self, procs: dict) -> None:
        super().__init__()
        self.keys = sorted(procs.keys())
        self.values = torch.nn.ModuleList(procs[k] for k in self.keys)

    def __getitem__(self, key):
        return self.values[self.keys.index(key)]


'''class SuperNet(torch.nn.Module):
    def __init__(self, state_dict: Dict[str, torch.Tensor]):
        super().__init__()
        state_dict = OrderedDict((k, state_dict[k]) for k in sorted(state_dict.keys()))
        self.layers = torch.nn.ModuleList(state_dict.values())
        self.mapping = dict(enumerate(state_dict.keys()))
        self.rev_mapping = {v: k for k, v in enumerate(state_dict.keys())}

        # .processor for unet, .self_attn for text encoder
        self.split_keys = [".processor", ".self_attn"]

        # we add a hook to state_dict() and load_state_dict() so that the
        # naming fits with `unet.attn_processors`
        def map_to(module, state_dict, *args, **kwargs):
            new_state_dict = {}
            for key, value in state_dict.items():
                num = int(key.split(".")[1])  # 0 is always "layers"
                new_key = key.replace(f"layers.{num}", module.mapping[num])
                new_state_dict[new_key] = value

            return new_state_dict

        def remap_key(key, state_dict):
            for k in self.split_keys:
                if k in key:
                    return key.split(k)[0] + k
            return key.split('.')[0]

        def map_from(module, state_dict, *args, **kwargs):
            all_keys = list(state_dict.keys())
            for key in all_keys:
                replace_key = remap_key(key, state_dict)
                new_key = key.replace(replace_key, f"layers.{module.rev_mapping[replace_key]}")
                state_dict[new_key] = state_dict[key]
                del state_dict[key]

        self._register_state_dict_hook(map_to)
        self._register_load_state_dict_pre_hook(map_from, with_module=True)
'''

class Zero123PlusPipeline(diffusers.StableDiffusionPipeline):
    tokenizer: transformers.CLIPTokenizer
    text_encoder: transformers.CLIPTextModel
    vision_encoder: transformers.CLIPVisionModelWithProjection

    feature_extractor_clip: transformers.CLIPImageProcessor
    unet: UNet2DConditionModel
    scheduler: diffusers.schedulers.KarrasDiffusionSchedulers

    vae: AutoencoderKL
    ramping: nn.Linear

    feature_extractor_vae: transformers.CLIPImageProcessor

    depth_transforms_multi = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5])
    ])

    def __init__(
        self,
        vae: AutoencoderKL,
        text_encoder: CLIPTextModel,
        tokenizer: CLIPTokenizer,
        unet: UNet2DConditionModel,
        scheduler: KarrasDiffusionSchedulers,
        vision_encoder: transformers.CLIPVisionModelWithProjection,
        feature_extractor_clip: CLIPImageProcessor, 
        feature_extractor_vae: CLIPImageProcessor,
        ramping_coefficients: Optional[list] = None,
        safety_checker=None,
    ):
        DiffusionPipeline.__init__(self)

        self.register_modules(
            vae=vae, text_encoder=text_encoder, tokenizer=tokenizer,
            unet=unet, scheduler=scheduler, safety_checker=None,
            vision_encoder=vision_encoder,
            feature_extractor_clip=feature_extractor_clip,
            feature_extractor_vae=feature_extractor_vae
        )
        self.register_to_config(ramping_coefficients=ramping_coefficients)
        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)
        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)

    def prepare(self, skeleton_guidance=False):
        train_sched = DDPMScheduler.from_config(self.scheduler.config)
        if isinstance(self.unet, UNet2DConditionModel):
            self.unet = RefOnlyNoisedUNet(self.unet, train_sched, self.scheduler,
                                        skeleton_guidance=skeleton_guidance
                                        ).eval()

   

    def encode_condition_image(self, image: torch.Tensor):
        image = self.vae.encode(image).latent_dist.sample()
        return image
    
    def encode_skeleton_image(self, skeleton_image: torch.Tensor):
        """Encode skeleton image through VAE following Skel3D approach"""
        skeleton_latents = self.vae.encode(skeleton_image).latent_dist.sample()
        return skeleton_latents

    @torch.no_grad()
    def __call__(
        self,
        image: Image.Image = None,
        prompt = "",
        *args,
        num_images_per_prompt: Optional[int] = 1,
        guidance_scale=4.0,
        depth_image: Image.Image = None,
        skeleton_image: Image.Image = None,
        output_type: Optional[str] = "pil",
        width=640,
        height=960,
        num_inference_steps=28,
        return_dict=True,
        **kwargs
    ):
        self.prepare(skeleton_guidance=skeleton_image is not None)

        if image is None:
            raise ValueError("Inputting embeddings not supported for this pipeline. Please pass an image.")
        assert not isinstance(image, torch.Tensor)
        image = to_rgb_image(image)
        image_1 = self.feature_extractor_vae(images=image, return_tensors="pt").pixel_values
        image_2 = self.feature_extractor_clip(images=image, return_tensors="pt").pixel_values
        '''if depth_image is not None and hasattr(self.unet, "controlnet"):
            depth_image = to_rgb_image(depth_image)
            depth_image = self.depth_transforms_multi(depth_image).to(
                device=self.unet.controlnet.device, dtype=self.unet.controlnet.dtype
            )'''
        image = image_1.to(device=self.vae.device, dtype=self.vae.dtype)
        image_2 = image_2.to(device=self.vae.device, dtype=self.vae.dtype)
        cond_lat = self.encode_condition_image(image)
        
        # Process skeleton image if provided
        skel_lat = None
        if skeleton_image is not None:
            skeleton_image = to_rgb_image(skeleton_image)
            skeleton_image_tensor = self.feature_extractor_vae(images=skeleton_image, return_tensors="pt").pixel_values
            skeleton_image_tensor = skeleton_image_tensor.to(device=self.vae.device, dtype=self.vae.dtype)
            skel_lat = self.encode_skeleton_image(skeleton_image_tensor)
        
        if guidance_scale > 1:
            negative_lat = self.encode_condition_image(torch.zeros_like(image))
            cond_lat = torch.cat([negative_lat, cond_lat])
            
            # Handle skeleton latents for CFG
            if skel_lat is not None:
                negative_skel_lat = self.encode_skeleton_image(torch.zeros_like(skeleton_image_tensor))
                skel_lat = torch.cat([negative_skel_lat, skel_lat])
        encoded = self.vision_encoder(image_2, output_hidden_states=False)
        global_embeds = encoded.image_embeds
        global_embeds = global_embeds.unsqueeze(-2)
        
        if hasattr(self, "encode_prompt"):
            encoder_hidden_states = self.encode_prompt(
                prompt,
                self.device,
                num_images_per_prompt,
                False
            )[0]
        else:
            encoder_hidden_states = self._encode_prompt(
                prompt,
                self.device,
                num_images_per_prompt,
                False
            )
        ramp = global_embeds.new_tensor(self.config.ramping_coefficients).unsqueeze(-1)
        encoder_hidden_states = encoder_hidden_states + global_embeds * ramp
        cak = dict(cond_lat=cond_lat)
        if skel_lat is not None:
            cak['skel_latents'] = skel_lat
        if hasattr(self.unet, "controlnet"):
            cak['control_depth'] = depth_image
        latents: torch.Tensor = super().__call__(
            None,
            *args,
            cross_attention_kwargs=cak,
            guidance_scale=guidance_scale,
            num_images_per_prompt=num_images_per_prompt,
            prompt_embeds=encoder_hidden_states,
            num_inference_steps=num_inference_steps,
            output_type='latent',
            width=width,
            height=height,
            **kwargs
        ).images
        latents = unscale_latents(latents)
        if not output_type == "latent":
            image = unscale_image(self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0])
        else:
            image = latents

        image = self.image_processor.postprocess(image, output_type=output_type)
        if not return_dict:
            return (image,)

        return ImagePipelineOutput(images=image)